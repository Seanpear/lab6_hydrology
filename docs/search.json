[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lab 6 ML Workflows",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n# Download the CAMELS documentation PDF\ndownload.file(\n  url = \"https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf\",\n  destfile = \"data/camels_attributes_v2.0.pdf\",\n  mode = \"wb\"\n)\n# Define the file types we want\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# Root URL and file paths\nroot &lt;- \"https://gdex.ucar.edu/dataset/camels/file\"\nremote_files &lt;- glue(\"{root}/camels_{types}.txt\")\nlocal_files  &lt;- glue(\"data/camels_{types}.txt\")\n\n# Download each .txt file to the data folder\nwalk2(remote_files, local_files, ~ download.file(.x, .y, quiet = TRUE))\n# Read each .txt file \ncamels_list &lt;- map(local_files, ~ read_delim(.x, show_col_types = FALSE))\n# Merge all data frames \ncamels &lt;- power_full_join(camels_list, by = \"gauge_id\")"
  },
  {
    "objectID": "index.html#question-1",
    "href": "index.html#question-1",
    "title": "Lab 6 ML Workflows",
    "section": "Question 1:",
    "text": "Question 1:\n\nPart 1: From the documentation PDF, report what zero_q_freq represents\nThis represents the frequency of days with Q = 0 mm/day. In other words, the percentage of days in the streamflow record when streamflow was zero at the gauge."
  },
  {
    "objectID": "index.html#question-2",
    "href": "index.html#question-2",
    "title": "Lab 6 ML Workflows",
    "section": "Question 2:",
    "text": "Question 2:\n\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(maps)\n\nWarning: package 'maps' was built under R version 4.4.3\n\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(ggthemes)\n\nWarning: package 'ggthemes' was built under R version 4.4.3\n\n\n\n# Add US state borders\nbase_map &lt;- borders(\"state\", colour = \"gray50\")\n\n\nPart 1: Create Aridity Map\n\n# Map 1: Color by aridity\nmap_aridity &lt;- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) +\n  base_map +\n  geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"lightyellow\", high = \"darkred\") +\n  ggtitle(\"Site Aridity\") +\n  theme_map()\n\n\n\nPart 2: Create p_mean Map\n\n# Map 2: Color by p_mean \nmap_pmean &lt;- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) +\n  base_map +\n  geom_point(aes(color = p_mean)) +\n  scale_color_gradient(low = \"lightblue\", high = \"darkblue\") +\n  ggtitle(\"Mean Precipitation (p_mean)\") +\n  theme_map()\n\n\n\nPart 3: Combine the Maps\n\n# Combine the two plots sidebyside\nmap_aridity + map_pmean"
  },
  {
    "objectID": "index.html#question-3",
    "href": "index.html#question-3",
    "title": "Lab 6 ML Workflows",
    "section": "Question 3:",
    "text": "Question 3:\n\n# Load if not already\nlibrary(xgboost)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(ranger)\n\nWarning: package 'ranger' was built under R version 4.4.3\n\n\n\n# Make sure logQmean exists\ncamels &lt;- camels %&gt;%\n  mutate(logQmean = log(q_mean))\n\n\n# Split the data\nset.seed(123)\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\n\n# 10-fold CV\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\nrec &lt;- recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) %&gt;%\n  step_naomit(all_predictors(), all_outcomes())\n\n\n# Linear regression\nlm_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n\n# Random forest\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n\nPart 1: Build an xgboost Model\n\n# XGBoost\nxgb_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\n\nPart 2: Build a Neural Network Model\n\n# Neural Net via bagged MLP\nnnet_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\n\n\nPart 3: Add All Four Models to Workflow\n\nwf &lt;- workflow_set(\n  preproc = list(rec),\n  models = list(\n    linear_reg = lm_model,\n    random_forest = rf_model,\n    xgboost = xgb_model,\n    neural_net = nnet_model\n  )\n) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = camels_cv)\n\n\n\nPart 4: Compare Model Performance and Evaluation\n\n# Plot performance across models\nautoplot(wf)\n\n\n\n\n\n\n\n# Rank by R²\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_neural_net Prepro… rmse    0.557  0.0253    10 recipe       bag_…     1\n2 recipe_neural_net Prepro… rsq     0.782  0.0224    10 recipe       bag_…     1\n3 recipe_random_fo… Prepro… rmse    0.564  0.0254    10 recipe       rand…     2\n4 recipe_random_fo… Prepro… rsq     0.771  0.0262    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 recipe_xgboost    Prepro… rmse    0.600  0.0289    10 recipe       boos…     4\n8 recipe_xgboost    Prepro… rsq     0.745  0.0268    10 recipe       boos…     4\n\n\nModel Evaluation and Comparison I compared four models: linear regression, random forest, xgboost, and a neural network. The neural network model (bag_mlp) performed the best, with the lowest RMSE (0.557) and highest r-squared (0.782). This means it made the most accurate predictions and explained the most variation in the data. Random forest came in second, followed by linear regression. xgboost had the lowest performance. Based on this, I would choose the neural network model to move forward with.\n\n\nPart 5: Conclusion\nBased on my results, the model I should move forward with is bag_mlp."
  },
  {
    "objectID": "index.html#build-your-own",
    "href": "index.html#build-your-own",
    "title": "Lab 6 ML Workflows",
    "section": "Build Your Own:",
    "text": "Build Your Own:\n\nPart 1: Data Splitting\n\n# Set seed\nset.seed(42)  # New seed to show it’s a different split\n\n\n# Create log-transformed target variable\ncamels &lt;- camels %&gt;%\n  mutate(logQmean = log(q_mean))\n\n\n# New 75/25 split for Build Your Own section\ncamels_split_byo &lt;- initial_split(camels, prop = 0.75)\ncamels_train_byo &lt;- training(camels_split_byo)\ncamels_test_byo  &lt;- testing(camels_split_byo)\n\n\n# New 10-fold CV\ncamels_cv_byo &lt;- vfold_cv(camels_train_byo, v = 10)\n\n\n\nPart 2: Recipe\nI will be using the formula logQmean ~ p_mean + pet_mean + slope_mean + frac_forest. I chose this formula because these variables because they all affect how much water flows in a river:\n\np_mean = how much rain falls\n\npet_mean = how much water goes back into the air\n\nslope_mean = how steep the land is\n\nfrac_forest = how much of the land is covered by trees\n\nThese things all impact how much water ends up in the stream, so they help predict streamflow.\n\nrec_byo &lt;- recipe(logQmean ~ p_mean + pet_mean + slope_mean + frac_forest, data = camels_train_byo) %&gt;%\n  step_mutate(frac_forest = ifelse(is.finite(frac_forest), frac_forest, NA)) %&gt;%\n  step_naomit(all_predictors(), all_outcomes()) %&gt;%\n  step_normalize(all_predictors())\n\n\n\nPart 3: Define 3 Models\n\n# Random Forest model\nrf_model_byo &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n\n# XGBoost model\nxgb_model_byo &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\n# Neural Network (bagged MLP) model\nnnet_model_byo &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\n\n\nPart 4: Workflow Set\n\n# Create a named list of models\nmodels_byo &lt;- list(\n  random_forest = rf_model_byo,\n  xgboost = xgb_model_byo,\n  neural_net = nnet_model_byo\n)\n\n# Create a workflow set using the same recipe for all models\nwf_byo &lt;- workflow_set(\n  preproc = list(recipe = rec_byo),\n  models  = models_byo\n)\n\n# Fit all models to 10-fold CV resamples\nwf_byo_results &lt;- wf_byo %&gt;%\n  workflow_map(\"fit_resamples\", resamples = camels_cv_byo)\n\n\n\nPart 5: Evaluation\n\n# Visualize performance\nautoplot(wf_byo_results)\n\n\n\n\n\n\n\n# Show best model by R-squared\nrank_results(wf_byo_results, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_neural_net Prepro… rmse    0.334  0.0224    10 recipe       bag_…     1\n2 recipe_neural_net Prepro… rsq     0.912  0.0135    10 recipe       bag_…     1\n3 recipe_random_fo… Prepro… rmse    0.373  0.0202    10 recipe       rand…     2\n4 recipe_random_fo… Prepro… rsq     0.889  0.0156    10 recipe       rand…     2\n5 recipe_xgboost    Prepro… rmse    0.373  0.0221    10 recipe       boos…     3\n6 recipe_xgboost    Prepro… rsq     0.889  0.0140    10 recipe       boos…     3\n\n\nBased on the results I can conclude that bag_mlp is the most accurate model. This is due to the fact that it has the lowest rmse and highest r-squared. Low rmse values mean better predictions and r-squared values that are near 1 better fit the model.\n\n\nStep 6: Extract and Evaluate\n\n# Build a workflow with the best model and recipe\nfinal_wf &lt;- workflow() %&gt;%\n  add_model(nnet_model_byo) %&gt;%   # bag_mlp model you defined earlier\n  add_recipe(rec_byo)             # recipe you built earlier\n\n\n# Fit the model to all training data\nfinal_fit &lt;- fit(final_wf, data = camels_train_byo)\n\n\n# Predict on the test data\nfinal_preds &lt;- augment(final_fit, new_data = camels_test_byo)\n\n\n# Plot observed vs predicted values\nggplot(final_preds, aes(x = logQmean, y = .pred, color = frac_forest)) +\n  geom_point() +\n  geom_abline(linetype = \"dashed\", color = \"black\") +\n  scale_color_viridis_c() +\n  labs(\n    title = \"Observed vs Predicted Log Mean Streamflow (Bagged MLP)\",\n    x = \"Observed logQmean\",\n    y = \"Predicted logQmean\",\n    color = \"Fraction Forest\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe bagged MLP model did a great job predicting mean streamflow. Most of the points are close to the dashed line, which means the predictions are very similar to the real values. The color scale (Fraction Forest) also looks well spread out, so the model seems to work across different land cover types. Overall, the model looks accurate and should do well predicting streamflow in similar places."
  }
]